#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri May 10 15:03:32 2019

@author: Seahymn
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"]="3"
import datetime
import numpy as np
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.callbacks import TensorBoard, CSVLogger
from Functions import dump_data_train,LoadSavedData,JoinSubLists,SavedData,Generate_saved_path,plot_history
from sklearn.utils import class_weight

from BiGRU_model_GPU import BiGRU_network

#--------------------------------------------------------------------------------------------------
# Parameters used
#--------------------------------------------------------------------------------------------------

MAX_LEN = 1000 # The Padding Length for each sample.
EMBEDDING_DIM = 100 # The Embedding Dimension for each element within the sequence of a data sample. 
LOSS_FUNCTION = 'binary_crossentropy'
#OPTIMIZER = 'adamax'
OPTIMIZER = 'sgd'
BATCH_SIZE = 16
EPOCHS = 120
PATIENCE = 35

LoopN = 10

working_dir = './data/'
dump_data_dir = working_dir + '9_projects_Functions'
token_dir = './data/'

directory_path = working_dir + os.sep
Files_path = os.listdir(dump_data_dir)  

projects = []
projects_list = []   
         
for file in Files_path:
    projects.append(file)
    projects_list.append(file)  
for i in range(len(projects_list)):
    Start = 1
    test_list = projects_list[i]
    projects.remove(test_list)
    train_list = projects    
    
    model_name = 'BiGRU_cross_projects_predict_' + test_list
    
    saved_path = working_dir + model_name + os.sep
    model_saved_path = saved_path + 'models'
    log_path = saved_path + 'logs'
    model_saved_pkl_path = saved_path + 'pkl'
    model_saved_graph_path = saved_path + 'graph'
    
    #-----------------------------------------------------------------------------------------------------
    # 1. Load the data for training and validation
    #-----------------------------------------------------------------------------------------------------
    
    total_list,total_list_id,total_list_label = dump_data_train(dump_data_dir,train_list)
    
    #--------------------------------------------------------------------------------------------------
    # 2. Load the Tokenizer and the trained word2vec model
    #--------------------------------------------------------------------------------------------------
    
    # 2.1 Load the toknizer
    
    new_total_list =  JoinSubLists(total_list)
    
    tokenizer = LoadSavedData(token_dir + 'all_tokenizer_no_comments.pickle')
    
    total_sequences = tokenizer.texts_to_sequences(new_total_list)
    
    word_index = tokenizer.word_index
    print ('Found %s unique tokens.' % len(word_index))
    
    
    print ("The length of train tokenized sequence: " + str(len(total_sequences)))
    
    # 2.2 Load the trained word2vec model.
    
    w2v_model_path = token_dir + 'all_w2v_model_CBOW_no_comments.txt'
    w2v_model = open(w2v_model_path, encoding="utf-8")
    
    print ("-------------------------------------------------------")
    print ("The trained word2vec model: ")
    print (w2v_model)
    
    #----------------------------------------------------------------------------------------------------
    # 3. Do the paddings.
    #----------------------------------------------------------------------------------------------------
    
    print ("max_len ", MAX_LEN)
    print('Pad sequences (samples x time)')
    
    total_sequences_pad = pad_sequences(total_sequences, maxlen = MAX_LEN, padding ='post')
    
    print ("The shape after paddings: ")
    print (total_sequences_pad.shape)
    
    train_set_x, validation_set_x, train_set_y, validation_set_y, train_set_id, validation_set_id = train_test_split(total_sequences_pad, total_list_label, total_list_id, test_size=0.3, random_state=42)
    
    train_set_y = np.asarray(train_set_y)
    validation_set_y = np.asarray(validation_set_y)
   
    #----------------------------------------------------------------------------------------------------
    # 4. Preparing the Embedding layer
    #----------------------------------------------------------------------------------------------------
    
    class_weights = class_weight.compute_class_weight('balanced',
                                                      np.unique(train_set_y),
                                                      train_set_y)
    
    embeddings_index = {} # a dictionary with mapping of a word i.e. 'int' and its corresponding 100 dimension embedding.
    
    # Use the loaded model
    for line in w2v_model:
       if not line.isspace():
           values = line.split()
           word = values[0]
           coefs = np.asarray(values[1:], dtype='float32')
           embeddings_index[word] = coefs
    w2v_model.close()
    
    print('Found %s word vectors.' % len(embeddings_index))
    
    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
       embedding_vector = embeddings_index.get(word)
       if embedding_vector is not None:
           # words not found in embedding index will be all-zeros.
           embedding_matrix[i] = embedding_vector
           
    #----------------------------------------------------------------------------------------------------
    # 5. Train model function
    #----------------------------------------------------------------------------------------------------
           
    def train(train_set_x, train_set_y, validation_set_x, validation_set_y, saved_model_name, Version):    
    
        model = BiGRU_network(MAX_LEN, EMBEDDING_DIM, word_index, embedding_matrix, True)

        callbacks_list = [
                ModelCheckpoint(filepath = model_saved_path + os.sep + Version + saved_model_name +'_{epoch:02d}_{val_acc:.3f}_{val_loss:3f}' + '.h5', monitor='val_loss', verbose=1, save_best_only=True, period=1),
                EarlyStopping(monitor='val_loss', patience=PATIENCE, verbose=1, mode="min"),
                TensorBoard(log_dir=log_path, batch_size = BATCH_SIZE,  write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None),
                CSVLogger(log_path + os.sep + saved_model_name + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.log')]
    
        train_history = model.fit(train_set_x, train_set_y,
                                  epochs = EPOCHS,
                                  batch_size = BATCH_SIZE,
                                  shuffle = False, # The data has already been shuffle before, so it is unnessary to shuffle it again. (And also, we need to correspond the ids to the features of the samples.)
                                  validation_data = (validation_set_x, validation_set_y), # Validation data is not used for training (or development of the model)
                                  callbacks = callbacks_list, # Get the best weights of the model and stop the first raound training.
                                  verbose = 1,
                                  class_weight = class_weights)    
        model.summary()    
        return model, train_history
    
    #----------------------------------------------------------------------------------------------------
    # 6. Preparing the Embedding layer
    #----------------------------------------------------------------------------------------------------
    print ('----------'+ model_name + '----------')
    
    Generate_saved_path(working_dir,model_name)
    for i in range(LoopN):
        Start = Start + 1
        Version = str(Start) + '_'
        print(Version + 'th Training:')
        model, train_history = train(train_set_x, train_set_y, validation_set_x, validation_set_y, model_name,Version)
        plot_history(train_history,model_saved_graph_path, Version, model_name)  
        SavedData(model_saved_pkl_path + os.sep + Version + model_name + '.pkl', train_history)
        
    projects.append(test_list)