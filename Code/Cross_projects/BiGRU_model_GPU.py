# -*- coding: utf-8 -*-
"""
Created on May 14 2019

@author: Seahymn

Bi-GRU model

"""

LOSS_FUNCTION = 'binary_crossentropy'
#OPTIMIZER = 'adamax'
OPTIMIZER = 'sgd'

from keras.models import Model
from keras.layers import Input, Dense, Embedding, Bidirectional, GlobalMaxPooling1D, CuDNNGRU
from keras.layers.core import Dropout

def BiGRU_network(MAX_LEN, EMBEDDING_DIM, word_index, embedding_matrix, use_dropout=False):
    inputs = Input(shape=(MAX_LEN,))

    sharable_embedding = Embedding(len(word_index) + 1,
                               EMBEDDING_DIM,
                               weights=[embedding_matrix],
                               input_length=MAX_LEN,
                               trainable=False)(inputs)
    bigru_1 = Bidirectional(CuDNNGRU(64, return_sequences=True), merge_mode='concat')(sharable_embedding) # The default activation is 'tanh',
    if use_dropout:
        droput_layer_1 = Dropout(0.5)(bigru_1)
        bigru_2 = Bidirectional(CuDNNGRU(64, return_sequences=True), merge_mode='concat')(droput_layer_1)
    else:
        bigru_2 = Bidirectional(CuDNNGRU(64, return_sequences=True), merge_mode='concat')(bigru_1)
    
    gmp_layer = GlobalMaxPooling1D()(bigru_2)
    
    if use_dropout:
        dropout_layer_2 = Dropout(0.5)(gmp_layer)
        dense_1 = Dense(64, activation='relu')(dropout_layer_2)
    else:
        dense_1 = Dense(64, activation='relu')(gmp_layer)
        
    dense_2 = Dense(32)(dense_1)
    dense_3 = Dense(1, activation='sigmoid')(dense_2)
    
    model = Model(inputs=inputs, outputs = dense_3, name='BiGRU_network')
    
    model.compile(loss=LOSS_FUNCTION,
             optimizer=OPTIMIZER,
             metrics=['accuracy'])
    
    return model